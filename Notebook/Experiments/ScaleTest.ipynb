{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530bcadd-d54c-45c1-a2eb-c050957af468",
   "metadata": {},
   "source": [
    "# Scale Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f51a09-9ad9-4af6-9e8c-9e573c5e1671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.trainer import CaseDataSet\n",
    "from src.model import DLModels\n",
    "from src.trainer import Trainer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import importlib.util\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895cd2f8-0706-44e0-be03-ae8448d0a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scale_test_model(datasets_dict, model_hyper_para, trainer_hyper_para,\n",
    "                       scale, embedding, print_iter=False, file_suffix=\"\"):\n",
    "    torch_device = \"cpu\"\n",
    "    device_package = torch.cpu\n",
    "    if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch_device = torch.device(\"mps\")\n",
    "            device_package = torch.mps\n",
    "    if torch.cuda.is_available():\n",
    "        torch_device = torch.device(\"cuda\")\n",
    "        device_package = torch.cuda\n",
    "        \n",
    "    current_model_para = model_hyper_para[embedding]\n",
    "    # Instantiate the model\n",
    "    model = DLModels.SimpleLSTM(current_model_para[\"input_size\"],\n",
    "                                current_model_para[\"hidden_size\"],\n",
    "                                current_model_para[\"num_layers\"],\n",
    "                                1).to(torch_device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=trainer_hyper_para[\"learning_rate\"])\n",
    "    \n",
    "    # Load data sets\n",
    "    target_data_sets = datasets_dict[scale][embedding]\n",
    "    \n",
    "\n",
    "    \n",
    "    # print(torch_device)\n",
    "    # print(\"training model with target data\")\n",
    "    model_source, train_loss_source, val_loss_srouce = Trainer.train_model(model, optimizer,\n",
    "                                                                           None, None,\n",
    "                                                                           target_data_sets[\"train\"],\n",
    "                                                                           target_data_sets[\"val\"],\n",
    "                                                                           trainer_hyper_para[\"batch_size\"],\n",
    "                                                                           torch_device,\n",
    "                                                                           device_package,\n",
    "                                                                           Trainer.prefix_weighted_loss,\n",
    "                                                                           trainer_hyper_para[\"max_epoch\"],\n",
    "                                                                           trainer_hyper_para[\"max_ob_iter\"],\n",
    "                                                                           trainer_hyper_para[\"score_margin\"],\n",
    "                                                                           print_iter=print_iter)\n",
    "    \n",
    "    \n",
    "    model_name = \"LSTM_T_h\" + str(current_model_para[\"hidden_size\"]) + \"_l\" + str(current_model_para[\"num_layers\"]) + \"_\" + embedding + \"_\" + scale\n",
    "    torch.save(model_source, \"../../Model/scale/LSTM/\"+ model_name + \"_\" +  file_suffix + \".LSTM\")\n",
    "    \n",
    "    training_stat = pd.DataFrame(columns=[\"TrainingLoss\", \"ValidationLoss\"],\n",
    "                                 data=np.hstack([train_loss_source.reshape((-1, 1)),\n",
    "                                                 val_loss_srouce.reshape((-1, 1))]))\n",
    "    training_stat.to_pickle(\"../../Model/scale/LSTM/\"+ model_name + \"_\" + file_suffix + \"_stat.pkl\")\n",
    "    \n",
    "    \n",
    "    # print(\"Finished training model with target data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af8f35c-ccbd-4520-bdcf-4919453f42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hyper_para = {\"max_epoch\": 100,\n",
    "                      \"max_ob_iter\": 20,\n",
    "                      \"score_margin\": 1,\n",
    "                      \"num_class\": 1,\n",
    "                      \"num_layers\": 1,\n",
    "                      \"learning_rate\": 1e-3,\n",
    "                      \"batch_size\": 4000,\n",
    "                      \"training_loss_func\": \"CCM\",\n",
    "                      \"eval_loss_func\": \"CCM\"}\n",
    "\n",
    "model_hyper_para = {\"w2v\": {\"input_size\": 51,\n",
    "                            \"hidden_size\": 128,\n",
    "                            \"num_layers\": 1},\n",
    "                    \"st\": {\"input_size\": 385,\n",
    "                           \"hidden_size\": 512,\n",
    "                           \"num_layers\": 1},\n",
    "                    \"onehot\": {\"input_size\": 48,\n",
    "                               \"hidden_size\": 128,\n",
    "                               \"num_layers\": 1}}\n",
    "\n",
    "\n",
    "scale_ratio = [\"0.01\" ,\"0.05\", \"0.1\", \"0.5\"]\n",
    "data_set_type = [\"source\", \"target\"]\n",
    "data_set_cat = [\"train\", \"val\"]\n",
    "embedding_type = [\"w2v\", \"st\", \"onehot\"]\n",
    "dataset_list_index = [scale_ratio, embedding_type, data_set_type, data_set_cat]\n",
    "\n",
    "earliness_requirement = True\n",
    "folder_path = \"../../Data/Training/scale/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e15e02-89e8-4f1e-b867-e93be02db4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data scale:  0.01\n",
      "loading data scale:  0.05\n",
      "loading data scale:  0.1\n",
      "loading data scale:  0.5\n"
     ]
    }
   ],
   "source": [
    "data_sets_list = {}\n",
    "for scale_num in scale_ratio:\n",
    "    print(\"loading data scale: \", scale_num)\n",
    "    data_scale_list = {}\n",
    "    for embedding in embedding_type:\n",
    "        data_embedding_list = {}\n",
    "        for d_cat in data_set_cat:       \n",
    "            data_version = \"_\" + d_cat + \"_\" + scale_num\n",
    "            embedding_version = \"_\" + embedding                \n",
    "            case_data_set = CaseDataSet.CaseDataset(project_data_path=folder_path,\n",
    "                                                    split_pattern=\"\",\n",
    "                                                    input_data=\"target\",\n",
    "                                                    data_version=data_version,\n",
    "                                                    embedding_version=embedding_version,\n",
    "                                                    earliness_requirement=earliness_requirement)\n",
    "            data_embedding_list[d_cat] = case_data_set\n",
    "        data_scale_list[embedding] = data_embedding_list\n",
    "    data_sets_list[scale_num] = data_scale_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a797ac-b68b-4308-8b00-b8fe8b091020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration:  6\n",
      "Training iteration:  7\n",
      "Training iteration:  8\n",
      "Training iteration:  9\n"
     ]
    }
   ],
   "source": [
    "for i in range(6,10):\n",
    "    print(\"Training iteration: \", i)\n",
    "    train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           \"0.5\", \"w2v\", print_iter=False, file_suffix=\"r\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ff0b1-5609-43d7-addc-0c57f86895e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,10):\n",
    "    print(\"Training iteration: \", i)\n",
    "    train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           \"0.5\", \"st\", print_iter=False, file_suffix=\"r\"+str(i))\n",
    "    train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           \"0.5\", \"onehot\", print_iter=False, file_suffix=\"r\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eae5dc5-1025-4b82-b9e0-fb3cce65ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration:  0\n",
      "Training iteration:  1\n",
      "Training iteration:  2\n",
      "Training iteration:  3\n",
      "Training iteration:  4\n",
      "Training iteration:  5\n",
      "Training iteration:  6\n",
      "Training iteration:  7\n",
      "Training iteration:  8\n",
      "Training iteration:  9\n",
      "Training iteration:  0\n",
      "Training iteration:  1\n",
      "Training iteration:  2\n",
      "Training iteration:  3\n",
      "Training iteration:  4\n",
      "Training iteration:  5\n",
      "Training iteration:  6\n",
      "Training iteration:  7\n",
      "Training iteration:  8\n",
      "Training iteration:  9\n",
      "Training iteration:  0\n",
      "Training iteration:  1\n",
      "Training iteration:  2\n",
      "Training iteration:  3\n",
      "Training iteration:  4\n",
      "Training iteration:  5\n",
      "Training iteration:  6\n",
      "Training iteration:  7\n",
      "Training iteration:  8\n",
      "Training iteration:  9\n",
      "Training iteration:  0\n",
      "Training iteration:  1\n",
      "Training iteration:  2\n",
      "Training iteration:  3\n",
      "Training iteration:  4\n",
      "Training iteration:  5\n",
      "Training iteration:  6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining iteration: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain_scale_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_sets_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_hyper_para\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mscale_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n\u001b[1;32m      7\u001b[0m                        scale_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw2v\u001b[39m\u001b[38;5;124m\"\u001b[39m, print_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, file_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m      8\u001b[0m     train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n\u001b[1;32m      9\u001b[0m                        scale_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m, print_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, file_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i))\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mtrain_scale_test_model\u001b[0;34m(datasets_dict, model_hyper_para, trainer_hyper_para, scale, embedding, print_iter, file_suffix)\u001b[0m\n\u001b[1;32m     23\u001b[0m target_data_sets \u001b[38;5;241m=\u001b[39m datasets_dict[scale][embedding]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(torch_device)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"training model with target data\")\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model_source, train_loss_source, val_loss_srouce \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtarget_data_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtarget_data_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mdevice_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_weighted_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_ob_iter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore_margin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mprint_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM_T_h\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_model_para[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_l\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_model_para[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m embedding \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m scale\n\u001b[1;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Model/scale/LSTM/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m  file_suffix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/Trainer.py:85\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, criterion_eval, training_set, val_set, batch_size, torch_device, device_package, eval_func, max_epoch, max_ob_iter, score_margin, print_iter)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[1;32m     84\u001b[0m     device_package\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 85\u001b[0m     loss_train, sample_num_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     device_package\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     88\u001b[0m     loss_val, sample_num_val \u001b[38;5;241m=\u001b[39m train_model_epoch(model, val_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     89\u001b[0m                                                  criterion\u001b[38;5;241m=\u001b[39mcriterion_eval, torch_device\u001b[38;5;241m=\u001b[39mtorch_device, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/Trainer.py:68\u001b[0m, in \u001b[0;36mtrain_model_epoch\u001b[0;34m(model, training_set, optimizer, criterion, torch_device, batch_size, training)\u001b[0m\n\u001b[1;32m     66\u001b[0m         x \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m][batch_size \u001b[38;5;241m*\u001b[39m batch_num :]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(torch_device)\n\u001b[1;32m     67\u001b[0m         y \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m1\u001b[39m][batch_size \u001b[38;5;241m*\u001b[39m batch_num :]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(torch_device)\n\u001b[0;32m---> 68\u001b[0m         loss_prefix \u001b[38;5;241m=\u001b[39m \u001b[43mforward_model_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     loss_prefix_list\u001b[38;5;241m.\u001b[39mappend(loss_prefix)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(loss_prefix_list), np\u001b[38;5;241m.\u001b[39marray(sample_num_list)\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/Trainer.py:37\u001b[0m, in \u001b[0;36mforward_model_batch\u001b[0;34m(model, x, y, optimizer, criterion, loss_prefix, torch_device, training)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m---> 37\u001b[0m loss_prefix \u001b[38;5;241m=\u001b[39m loss_prefix \u001b[38;5;241m+\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_prefix\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for scale_num in scale_ratio:\n",
    "    for i in range(10):\n",
    "        print(\"Training iteration: \", i)\n",
    "        train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           scale_num, \"st\", print_iter=False, file_suffix=\"r\"+str(i))\n",
    "        train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           scale_num, \"w2v\", print_iter=False, file_suffix=\"r\"+str(i))\n",
    "        train_scale_test_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                           scale_num, \"onehot\", print_iter=False, file_suffix=\"r\"+str(i))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
