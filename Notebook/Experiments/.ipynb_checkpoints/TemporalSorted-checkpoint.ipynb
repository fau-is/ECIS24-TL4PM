{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6fa5ac-f15f-4d10-a7cb-b4310c563436",
   "metadata": {},
   "source": [
    "# Experiments with temporal sorted data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e44dbc-9b0e-4565-9b05-1b05f1d67c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.trainer import CaseDataSet\n",
    "from src.model import DLModels\n",
    "from src.trainer import Trainer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import importlib.util\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b0e42c-feef-4c4e-bfc0-c65b4b0bf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scale_test_model(datasets_dict, model_hyper_para, trainer_hyper_para,\n",
    "                       scale, embedding, print_iter=False, file_suffix=\"\"):\n",
    "    torch_device = \"cpu\"\n",
    "    device_package = torch.cpu\n",
    "    if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch_device = torch.device(\"mps\")\n",
    "            device_package = torch.mps\n",
    "    if torch.cuda.is_available():\n",
    "        torch_device = torch.device(\"cuda\")\n",
    "        device_package = torch.cuda\n",
    "        \n",
    "    current_model_para = model_hyper_para[embedding]\n",
    "    # Instantiate the model\n",
    "    model = DLModels.SimpleLSTM(current_model_para[\"input_size\"],\n",
    "                                current_model_para[\"hidden_size\"],\n",
    "                                current_model_para[\"num_layers\"],\n",
    "                                1).to(torch_device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=trainer_hyper_para[\"learning_rate\"])\n",
    "    \n",
    "    # Load data sets\n",
    "    source_data_sets = datasets_dict[scale][embedding][\"source\"]\n",
    "    target_data_sets = datasets_dict[scale][embedding][\"target\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    print(torch_device)\n",
    "    print(\"training model with target data\")\n",
    "    model_source, train_loss_source, val_loss_srouce = Trainer.train_model(model, optimizer,\n",
    "                                                                           None, None,\n",
    "                                                                           target_data_sets[\"train\"],\n",
    "                                                                           target_data_sets[\"val\"],\n",
    "                                                                           trainer_hyper_para[\"batch_size\"],\n",
    "                                                                           torch_device,\n",
    "                                                                           device_package,\n",
    "                                                                           Trainer.prefix_weighted_loss,\n",
    "                                                                           trainer_hyper_para[\"max_epoch\"],\n",
    "                                                                           trainer_hyper_para[\"max_ob_iter\"],\n",
    "                                                                           trainer_hyper_para[\"score_margin\"],\n",
    "                                                                           print_iter=print_iter)\n",
    "    \n",
    "    \n",
    "    model_name = \"LSTM_T_h\" + str(current_model_para[\"hidden_size\"]) + \"_l\" + str(current_model_para[\"num_layers\"]) + \"_\" + embedding + \"_\" + scale\n",
    "    torch.save(model_source, \"../../Model/\" + split + \"/LSTM/\"+ model_name + \"_\" +  file_suffix + \".LSTM\")\n",
    "    \n",
    "    training_stat = pd.DataFrame(columns=[\"TrainingLoss\", \"ValidationLoss\"],\n",
    "                                 data=np.hstack([train_loss_source.reshape((-1, 1)),\n",
    "                                                 val_loss_srouce.reshape((-1, 1))]))\n",
    "    training_stat.to_pickle(\"../../Model/\" + split + \"/LSTM/\"+ model_name + \"_\" + file_suffix + \"_stat.pkl\")\n",
    "    \n",
    "    \n",
    "    print(\"Finished training model with target data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc9f4d8-56c2-4ee5-bac8-9864dc540ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hyper_para = {\"max_epoch\": 100,\n",
    "                      \"max_ob_iter\": 20,\n",
    "                      \"score_margin\": 1,\n",
    "                      \"num_class\": 1,\n",
    "                      \"num_layers\": 1,\n",
    "                      \"learning_rate\": 1e-3,\n",
    "                      \"batch_size\": 4000,\n",
    "                      \"training_loss_func\": \"CCM\",\n",
    "                      \"eval_loss_func\": \"CCM\"}\n",
    "\n",
    "model_hyper_para = {\"w2v\": {\"input_size\": 51,\n",
    "                            \"hidden_size\": 128,\n",
    "                            \"num_layers\": 1},\n",
    "                    \"st\": {\"input_size\": 385,\n",
    "                           \"hidden_size\": 512,\n",
    "                           \"num_layers\": 1},\n",
    "                    \"onehot\": {\"input_size\": 48,\n",
    "                               \"hidden_size\": 128,\n",
    "                               \"num_layers\": 1}}\n",
    "\n",
    "\n",
    "scale_ratio = [\"0.01\" ,\"0.05\", \"0.1\", \"0.5\"]\n",
    "data_set_type = [\"source\", \"target\"]\n",
    "data_set_cat = [\"train\", \"val\", \"test\"]\n",
    "embedding_type = [\"w2v\", \"st\", \"onehot\"]\n",
    "dataset_list_index = [split_pattern, embedding_type, data_set_type, data_set_cat]\n",
    "\n",
    "earliness_requirement = True\n",
    "folder_path = \"../../Data/Training/scale/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd46da1-9dab-4164-9af6-d5bfa581b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets_list = {}\n",
    "for scale_num in scale_ratio:\n",
    "    data_scale_list = {}\n",
    "    for embedding in embedding_type:\n",
    "        data_embedding_list = {}\n",
    "        for d_type in data_set_type:\n",
    "            data_type_list = {}\n",
    "            for d_cat in data_set_cat:       \n",
    "                data_version = \"_\" + d_cat + scale_num\n",
    "                embedding_version = \"_\" + embedding\n",
    "                case_data_set = CaseDataSet.CaseDataset(split_pattern=\"\",\n",
    "                                                        input_data=d_type,\n",
    "                                                        data_version=data_version,\n",
    "                                                        embedding_version=embedding_version,\n",
    "                                                        earliness_requirement=earliness_requirement)\n",
    "                \n",
    "                data_type_list[d_cat] = case_data_set\n",
    "            data_embedding_list[d_type] = data_type_list\n",
    "        data_scale_list[embedding] = data_embedding_list\n",
    "    data_sets_list[scale_num] = data_split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c3fe0b0-8589-4a4e-8868-f9c7ca8b8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "training model with source data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_source_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_sets_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_hyper_para\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m641620split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw2v\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mtrain_source_model\u001b[0;34m(datasets_dict, model_hyper_para, trainer_hyper_para, split, embedding, print_iter, file_suffix)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch_device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining model with source data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m model_source, train_loss_source, val_loss_srouce \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43msource_data_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43msource_data_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mdevice_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_weighted_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_ob_iter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtrainer_hyper_para\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore_margin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mprint_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM_S_h\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_model_para[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_l\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_model_para[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m embedding\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Model/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m split \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/LSTM/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m  file_suffix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/Trainer.py:83\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, criterion_eval, training_set, val_set, batch_size, torch_device, device_package, eval_func, max_epoch, max_ob_iter, score_margin, print_iter)\u001b[0m\n\u001b[1;32m     81\u001b[0m best_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     82\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[1;32m     84\u001b[0m     device_package\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     85\u001b[0m     loss_train, sample_num_train \u001b[38;5;241m=\u001b[39m train_model_epoch(model, training_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     86\u001b[0m                                                      criterion\u001b[38;5;241m=\u001b[39mcriterion, torch_device\u001b[38;5;241m=\u001b[39mtorch_device)\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/Trainer.py:45\u001b[0m, in \u001b[0;36mtrain_model_epoch\u001b[0;34m(model, training_set, optimizer, criterion, torch_device, batch_size, training)\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     44\u001b[0m loss_prefix_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 45\u001b[0m sample_num_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, training_data_set\u001b[38;5;241m.\u001b[39mmax_case_len):\n\u001b[1;32m     47\u001b[0m     loss_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Projects/ECIS24-TL4PM/src/trainer/CaseDataSet.py:76\u001b[0m, in \u001b[0;36mCaseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     73\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m data_temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_feature_vec_np)\u001b[38;5;241m.\u001b[39mvalues[idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 76\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdata_temp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tl4pm/lib/python3.8/site-packages/pandas/core/frame.py:3726\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3721\u001b[0m is_mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex)\n\u001b[1;32m   3722\u001b[0m \u001b[38;5;66;03m# GH#45316 Return view if key is not duplicated\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[38;5;66;03m# Only use drop_duplicates with duplicates for performance\u001b[39;00m\n\u001b[1;32m   3724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   3725\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique\n\u001b[0;32m-> 3726\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m   3727\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop_duplicates(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3728\u001b[0m ):\n\u001b[1;32m   3729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(key)\n\u001b[1;32m   3731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/anaconda3/envs/tl4pm/lib/python3.8/site-packages/pandas/core/indexes/base.py:5146\u001b[0m, in \u001b[0;36mIndex.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5144\u001b[0m \u001b[38;5;28mhash\u001b[39m(key)\n\u001b[1;32m   5145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\n\u001b[1;32m   5147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOverflowError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   5148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_source_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                   split=\"641620split\", embedding=\"w2v\", print_iter=True, file_suffix=\"b4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23184d0a-a360-4652-b60e-79fe85663c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "training model with target data\n",
      "Finished training iteration:  0  with val loss:  1605.7518292367765  train loss:  4641.883738829023\n",
      "Finished training iteration:  1  with val loss:  1933.3408428704852  train loss:  4621.871299030701\n",
      "Finished training iteration:  2  with val loss:  2882.726259362032  train loss:  4748.084772688445\n",
      "Finished training iteration:  3  with val loss:  2797.034042922885  train loss:  4961.521449720645\n",
      "Finished training iteration:  4  with val loss:  1851.7911332158924  train loss:  5427.397107133168\n",
      "Finished training iteration:  5  with val loss:  1483.8515063440937  train loss:  4834.286608756785\n",
      "Finished training iteration:  6  with val loss:  1919.6636668705642  train loss:  4720.191851888414\n",
      "Finished training iteration:  7  with val loss:  1605.63913459526  train loss:  4804.7747296750495\n",
      "Finished training iteration:  8  with val loss:  1535.8343948187937  train loss:  4532.64257838454\n",
      "Finished training iteration:  9  with val loss:  1645.7882266732217  train loss:  4537.14519007432\n"
     ]
    }
   ],
   "source": [
    "train_target_model(data_sets_list, model_hyper_para, trainer_hyper_para,\n",
    "                   split=\"641620split\", embedding=\"onehot\", print_iter=True, file_suffix=\"b2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
