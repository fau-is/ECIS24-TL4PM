{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de7c8c-01be-4e81-b457-be18d22d0981",
   "metadata": {},
   "source": [
    "# Manual single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee86884e-827e-45a0-97c8-456d20810ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.trainer import CaseDataSet\n",
    "from src.model import DLModels\n",
    "from src.trainer import Trainer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import importlib.util\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82498c5f-3882-47c6-916c-f17bd46a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_set, torch_device, device_package, decision_boundary=0.5):\n",
    "    model.flatten()\n",
    "    res, ref, num = Trainer.evaluate_model(model, test_set, torch_device, device_package)\n",
    "    res_prob = np.squeeze(torch.concat(res).numpy())\n",
    "    res_class = copy.copy(res_prob)\n",
    "    res_class[res_class < decision_boundary] = 0\n",
    "    res_class[res_class >= decision_boundary] = 1\n",
    "    ref_class = np.squeeze(torch.concat(ref).numpy()).astype(int)\n",
    "    roc_auc = roc_auc_score(ref_class, res_prob)\n",
    "    f1 = f1_score(ref_class, res_class)\n",
    "    f1_inverse = f1_score(1-ref_class, 1-res_class)\n",
    "    precision = precision_score(ref_class, res_class)\n",
    "    precision_inverse = precision_score(1-ref_class, 1-res_class)\n",
    "    recall = recall_score(ref_class, res_class)\n",
    "    recall_inverse = recall_score(1-ref_class, 1-res_class)\n",
    "    print(\"roc_auc: \", roc_auc)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"f1 inverse: \", f1_inverse)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Precision inverse: \", precision_inverse)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Recall inverse: \", recall_inverse)\n",
    "    return roc_auc, f1, f1_inverse, precision, precision_inverse, recall, recall_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe758624-3ebf-4474-a222-642be5f150d3",
   "metadata": {},
   "source": [
    "## Random split w2v with time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0310d1-8d86-4a9d-8fa8-b9ba2438c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6010546-eb97-4b48-a3a4-5178b818be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  996.6249177235654  train loss:  2660.7734584003942\n",
      "Finished training iteration:  1  with val loss:  1196.3048524362528  train loss:  2589.2944925410757\n",
      "Finished training iteration:  2  with val loss:  855.5405496474328  train loss:  2562.5796381589416\n",
      "Finished training iteration:  3  with val loss:  781.9919834990015  train loss:  2532.4193045895504\n",
      "Finished training iteration:  4  with val loss:  823.1721990579153  train loss:  2592.4140822618606\n",
      "Finished training iteration:  5  with val loss:  711.3471627569812  train loss:  2525.1250427588975\n",
      "Finished training iteration:  6  with val loss:  784.2330404241188  train loss:  2521.490577040124\n",
      "Finished training iteration:  7  with val loss:  700.9396956267996  train loss:  2518.2232184554523\n",
      "Finished training iteration:  8  with val loss:  695.8953017310303  train loss:  2496.858220715114\n",
      "Finished training iteration:  9  with val loss:  798.6070163065406  train loss:  2499.077705444793\n",
      "Finished training iteration:  10  with val loss:  771.9426358102133  train loss:  2534.8377576765874\n",
      "Finished training iteration:  11  with val loss:  940.5054510251233  train loss:  2557.881881320582\n",
      "Finished training iteration:  12  with val loss:  956.9672907939845  train loss:  2529.6885284238833\n",
      "Finished training iteration:  13  with val loss:  719.101321675683  train loss:  2558.9434581823753\n",
      "Finished training iteration:  14  with val loss:  755.7817586462152  train loss:  2514.2330697910206\n",
      "Finished training iteration:  15  with val loss:  705.7748758769479  train loss:  2526.0861783145715\n",
      "Finished training iteration:  16  with val loss:  661.1522509890392  train loss:  2511.521780533366\n",
      "Finished training iteration:  17  with val loss:  664.4067989390788  train loss:  2546.4340051570352\n",
      "Finished training iteration:  18  with val loss:  684.5039074747755  train loss:  2506.5514828792207\n",
      "Finished training iteration:  19  with val loss:  653.1601764973265  train loss:  2503.50588359288\n",
      "Finished training iteration:  20  with val loss:  698.3213613756637  train loss:  2486.7416958982626\n",
      "Finished training iteration:  21  with val loss:  966.2308303943373  train loss:  2498.0520028183346\n",
      "Finished training iteration:  22  with val loss:  697.6178610986657  train loss:  2539.6841680789657\n",
      "Finished training iteration:  23  with val loss:  643.1832442339795  train loss:  2531.268445694052\n",
      "Finished training iteration:  24  with val loss:  751.5890879863867  train loss:  2504.642936320522\n",
      "Finished training iteration:  25  with val loss:  659.1855821488125  train loss:  2509.7119861219962\n",
      "Finished training iteration:  26  with val loss:  660.7257111510133  train loss:  2498.8352666968935\n",
      "Finished training iteration:  27  with val loss:  673.4081723541907  train loss:  2502.2118139792165\n",
      "Finished training iteration:  28  with val loss:  673.372263692201  train loss:  2500.8672518494704\n",
      "Finished training iteration:  29  with val loss:  652.0835073140432  train loss:  2511.4476673829286\n",
      "Finished training iteration:  30  with val loss:  815.6640741253412  train loss:  2489.0128843951416\n",
      "Finished training iteration:  31  with val loss:  647.9904101167289  train loss:  2547.225403600917\n",
      "Finished training iteration:  32  with val loss:  645.1561118764772  train loss:  2535.4907270739222\n",
      "Finished training iteration:  33  with val loss:  644.6977591513645  train loss:  2488.293056870061\n",
      "Finished training iteration:  34  with val loss:  689.4964513231448  train loss:  2496.074039722024\n",
      "Finished training iteration:  35  with val loss:  677.3876082888288  train loss:  2503.901139551737\n",
      "Finished training iteration:  36  with val loss:  688.206190479529  train loss:  2528.3358818174875\n",
      "Finished training iteration:  37  with val loss:  642.8069155362065  train loss:  2485.2290803453498\n",
      "Finished training iteration:  38  with val loss:  721.3073677828228  train loss:  2530.7371799961884\n",
      "Finished training iteration:  39  with val loss:  666.1833867429152  train loss:  2512.6098425965915\n",
      "Finished training iteration:  40  with val loss:  620.0768082495354  train loss:  2511.6292986055596\n",
      "Finished training iteration:  41  with val loss:  625.1317846180839  train loss:  2489.3376842911043\n",
      "Finished training iteration:  42  with val loss:  685.363139593676  train loss:  2471.2820090503674\n",
      "Finished training iteration:  43  with val loss:  645.5290754279947  train loss:  2490.109812273905\n",
      "Finished training iteration:  44  with val loss:  642.1508758451655  train loss:  2486.4411959586573\n",
      "Finished training iteration:  45  with val loss:  660.6840951715428  train loss:  2487.087081545491\n",
      "Finished training iteration:  46  with val loss:  667.3296229256028  train loss:  2499.186855121334\n",
      "Finished training iteration:  47  with val loss:  642.357321680425  train loss:  2482.044228852515\n",
      "Finished training iteration:  48  with val loss:  625.2109043552204  train loss:  2480.624019843124\n",
      "Finished training iteration:  49  with val loss:  627.2281849416925  train loss:  2465.619775702411\n",
      "Finished training iteration:  50  with val loss:  633.843495158078  train loss:  2466.779654722464\n",
      "Finished training iteration:  51  with val loss:  642.91297762919  train loss:  2465.4401478537943\n",
      "Finished training iteration:  52  with val loss:  662.0562375384698  train loss:  2472.8804436466876\n",
      "Finished training iteration:  53  with val loss:  681.7165181980247  train loss:  2475.652344720467\n",
      "Finished training iteration:  54  with val loss:  634.2056720981907  train loss:  2473.1654636034973\n",
      "Finished training iteration:  55  with val loss:  620.3952883539118  train loss:  2463.491493127075\n",
      "Finished training iteration:  56  with val loss:  689.9076118461505  train loss:  2463.0352393954363\n",
      "Finished training iteration:  57  with val loss:  632.8577385973269  train loss:  2471.391818131532\n",
      "Finished training iteration:  58  with val loss:  633.5480258730413  train loss:  2483.969866567697\n",
      "Finished training iteration:  59  with val loss:  646.7535662611803  train loss:  2471.220648992159\n",
      "Finished training iteration:  60  with val loss:  675.1609553757918  train loss:  2471.720353752703\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 51  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d721b0-0ae2-4b8c-b2fa-5b1c3c37a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.788782648557422\n",
      "f1:  0.8300555396120669\n",
      "f1 inverse:  0.5439637599093998\n",
      "Precision:  0.8911160245025191\n",
      "Precision inverse:  0.4594780745389148\n",
      "Recall:  0.7768263397371082\n",
      "Recall inverse:  0.66651865008881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.788782648557422,\n",
       " 0.8300555396120669,\n",
       " 0.5439637599093998,\n",
       " 0.8911160245025191,\n",
       " 0.4594780745389148,\n",
       " 0.7768263397371082,\n",
       " 0.66651865008881)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a777e322-3716-4d11-b6ff-7e990c7a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.4230700553058763\n",
      "f1:  0.6871052202220479\n",
      "f1 inverse:  0.26430811386816866\n",
      "Precision:  0.6811446117192795\n",
      "Precision inverse:  0.2698606120139761\n",
      "Recall:  0.6931710707138267\n",
      "Recall inverse:  0.25897949929595954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4230700553058763,\n",
       " 0.6871052202220479,\n",
       " 0.26430811386816866,\n",
       " 0.6811446117192795,\n",
       " 0.2698606120139761,\n",
       " 0.6931710707138267,\n",
       " 0.25897949929595954)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b36b9-2645-40dc-8d02-ef542095d7ba",
   "metadata": {},
   "source": [
    "## Temporal split w2v with time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b0ade2-6ce0-4265-9ad3-9010c5e98d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84466f3-8750-482d-99f4-87a37685dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  770.7401489121513  train loss:  2111.58595620398\n",
      "Finished training iteration:  1  with val loss:  639.5801284197825  train loss:  1977.5015798619638\n",
      "Finished training iteration:  2  with val loss:  704.881448928058  train loss:  1964.4997446317514\n",
      "Finished training iteration:  3  with val loss:  668.2876379987465  train loss:  1975.0989536021889\n",
      "Finished training iteration:  4  with val loss:  614.1776659076711  train loss:  1978.8005905498533\n",
      "Finished training iteration:  5  with val loss:  795.8914965395888  train loss:  1952.002751499723\n",
      "Finished training iteration:  6  with val loss:  980.0886866191778  train loss:  1976.1910546374884\n",
      "Finished training iteration:  7  with val loss:  608.0032991130973  train loss:  2033.2781942557572\n",
      "Finished training iteration:  8  with val loss:  906.8084920839253  train loss:  1987.85975196129\n",
      "Finished training iteration:  9  with val loss:  669.5728545660515  train loss:  1981.9269394000075\n",
      "Finished training iteration:  10  with val loss:  678.8336844170116  train loss:  1981.2621113752766\n",
      "Finished training iteration:  11  with val loss:  604.8302387499945  train loss:  1984.1643644284716\n",
      "Finished training iteration:  12  with val loss:  847.0602758211553  train loss:  1948.2640666530451\n",
      "Finished training iteration:  13  with val loss:  651.9946966334588  train loss:  2009.4485381616007\n",
      "Finished training iteration:  14  with val loss:  622.8119700178228  train loss:  1989.5172907715519\n",
      "Finished training iteration:  15  with val loss:  889.9592930291369  train loss:  1949.6089641984058\n",
      "Finished training iteration:  16  with val loss:  740.100197328168  train loss:  1992.5161107114805\n",
      "Finished training iteration:  17  with val loss:  627.182992335422  train loss:  2004.922910037455\n",
      "Finished training iteration:  18  with val loss:  658.6224689908297  train loss:  1935.2599430661737\n",
      "Finished training iteration:  19  with val loss:  609.5564629040614  train loss:  1944.1946332572759\n",
      "Finished training iteration:  20  with val loss:  676.4145884720897  train loss:  1977.992008756913\n",
      "Finished training iteration:  21  with val loss:  674.7414857428332  train loss:  1957.4046189941673\n",
      "Finished training iteration:  22  with val loss:  639.8238698091476  train loss:  1940.0178965066239\n",
      "Finished training iteration:  23  with val loss:  629.7049019156059  train loss:  1970.5547724236471\n",
      "Finished training iteration:  24  with val loss:  659.5195500517867  train loss:  1938.9355086436642\n",
      "Finished training iteration:  25  with val loss:  563.8310089893132  train loss:  1965.3286099910365\n",
      "Finished training iteration:  26  with val loss:  556.2820939732803  train loss:  1951.6325317248438\n",
      "Finished training iteration:  27  with val loss:  586.1465136629738  train loss:  1937.1804376183995\n",
      "Finished training iteration:  28  with val loss:  614.9876613451455  train loss:  1932.6895211105978\n",
      "Finished training iteration:  29  with val loss:  570.7431729992064  train loss:  1961.18765172325\n",
      "Finished training iteration:  30  with val loss:  558.8593400351076  train loss:  1939.3319570924175\n",
      "Finished training iteration:  31  with val loss:  890.787908375165  train loss:  1943.3359674776113\n",
      "Finished training iteration:  32  with val loss:  580.9394407758762  train loss:  1985.800981533283\n",
      "Finished training iteration:  33  with val loss:  564.9902608475162  train loss:  1950.917817679336\n",
      "Finished training iteration:  34  with val loss:  635.2037524083211  train loss:  1916.6317042021506\n",
      "Finished training iteration:  35  with val loss:  608.0763116567258  train loss:  1921.4758986603022\n",
      "Finished training iteration:  36  with val loss:  638.9148062596626  train loss:  1922.1789993983057\n",
      "Finished training iteration:  37  with val loss:  691.0253358907896  train loss:  1923.6465590954922\n",
      "Finished training iteration:  38  with val loss:  641.3131409252311  train loss:  1941.9421891953116\n",
      "Finished training iteration:  39  with val loss:  568.405855407516  train loss:  1937.3223553133826\n",
      "Finished training iteration:  40  with val loss:  571.9681149830487  train loss:  1916.9840626488394\n",
      "Finished training iteration:  41  with val loss:  587.7893753688456  train loss:  1905.3982092354559\n",
      "Finished training iteration:  42  with val loss:  590.8207718423059  train loss:  1911.6872133160978\n",
      "Finished training iteration:  43  with val loss:  598.3714036519133  train loss:  1914.9654169372927\n",
      "Finished training iteration:  44  with val loss:  568.9011018833609  train loss:  1936.0861744196377\n",
      "Finished training iteration:  45  with val loss:  577.730751602623  train loss:  1910.1705729823768\n",
      "Finished training iteration:  46  with val loss:  554.101978644166  train loss:  1910.9070661310805\n",
      "Finished training iteration:  47  with val loss:  539.8677015169606  train loss:  1903.5947679978303\n",
      "Finished training iteration:  48  with val loss:  561.009098795834  train loss:  1897.0607900071977\n",
      "Finished training iteration:  49  with val loss:  580.3410907844551  train loss:  1895.8112682965927\n",
      "Finished training iteration:  50  with val loss:  563.0158816331747  train loss:  1905.3606489963845\n",
      "Finished training iteration:  51  with val loss:  558.0511832560867  train loss:  1900.5159801691066\n",
      "Finished training iteration:  52  with val loss:  560.4701990978974  train loss:  1897.066545558485\n",
      "Finished training iteration:  53  with val loss:  581.2955946157014  train loss:  1894.3918287192855\n",
      "Finished training iteration:  54  with val loss:  581.8202028929164  train loss:  1899.3718449486364\n",
      "Finished training iteration:  55  with val loss:  569.4383767363189  train loss:  1890.7262068354985\n",
      "Finished training iteration:  56  with val loss:  548.5496212956093  train loss:  1893.8758492618322\n",
      "Finished training iteration:  57  with val loss:  553.3863975009137  train loss:  1891.7121396578639\n",
      "Finished training iteration:  58  with val loss:  548.2869586897646  train loss:  1900.3186621531008\n",
      "Finished training iteration:  59  with val loss:  651.1856906177815  train loss:  1896.5759690161642\n",
      "Finished training iteration:  60  with val loss:  749.981760167649  train loss:  1905.7435979311847\n",
      "Finished training iteration:  61  with val loss:  602.1556236275349  train loss:  1936.0361958949704\n",
      "Finished training iteration:  62  with val loss:  588.6287202493295  train loss:  1906.3341134927098\n",
      "Finished training iteration:  63  with val loss:  636.6731042919832  train loss:  1895.803415326068\n",
      "Finished training iteration:  64  with val loss:  596.5694699669476  train loss:  1892.5602641549212\n",
      "Finished training iteration:  65  with val loss:  581.8191669793714  train loss:  1900.5963525199338\n",
      "Finished training iteration:  66  with val loss:  617.7484421299928  train loss:  1904.658136256158\n",
      "Finished training iteration:  67  with val loss:  668.2574562989437  train loss:  1895.1077236214353\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 51  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7eeb46-1b50-4fef-ae30-05549fad4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.7438235988684694\n",
      "f1:  0.8171005259090309\n",
      "f1 inverse:  0.5225136558240243\n",
      "Precision:  0.8626124333359866\n",
      "Precision inverse:  0.4592560553633218\n",
      "Recall:  0.7761504028648165\n",
      "Recall inverse:  0.6059810523912795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7438235988684694,\n",
       " 0.8171005259090309,\n",
       " 0.5225136558240243,\n",
       " 0.8626124333359866,\n",
       " 0.4592560553633218,\n",
       " 0.7761504028648165,\n",
       " 0.6059810523912795)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0945d41d-2e7b-4e3f-b772-03d5262eec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.644895825098191\n",
      "f1:  0.641076460933387\n",
      "f1 inverse:  0.45782850727417324\n",
      "Precision:  0.8065927001042286\n",
      "Precision inverse:  0.34949514788462055\n",
      "Recall:  0.531923504358607\n",
      "Recall inverse:  0.6634916868633798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.644895825098191,\n",
       " 0.641076460933387,\n",
       " 0.45782850727417324,\n",
       " 0.8065927001042286,\n",
       " 0.34949514788462055,\n",
       " 0.531923504358607,\n",
       " 0.6634916868633798)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83d3fbb3-a0ca-4b8d-840c-f5897ebe1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50ea6508-3883-426a-90b3-4068d5360527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  1988.9160514570456  train loss:  4317.641623615202\n",
      "Finished training iteration:  1  with val loss:  1682.0884687426717  train loss:  4183.40916337637\n",
      "Finished training iteration:  2  with val loss:  1820.4668025274987  train loss:  4113.818625999076\n",
      "Finished training iteration:  3  with val loss:  1666.8854319424315  train loss:  4013.5959644560708\n",
      "Finished training iteration:  4  with val loss:  1683.3796946949776  train loss:  3947.536043122505\n",
      "Finished training iteration:  5  with val loss:  1641.8601277235052  train loss:  3958.130681605511\n",
      "Finished training iteration:  6  with val loss:  1682.2979228832246  train loss:  3898.806011466092\n",
      "Finished training iteration:  7  with val loss:  1495.3503464992973  train loss:  3921.5271092744792\n",
      "Finished training iteration:  8  with val loss:  1443.6140393632006  train loss:  3919.21339096277\n",
      "Finished training iteration:  9  with val loss:  1450.4088309332203  train loss:  3852.6280342132545\n",
      "Finished training iteration:  10  with val loss:  1422.5088776529483  train loss:  3825.4753223426883\n",
      "Finished training iteration:  11  with val loss:  1391.6212728829937  train loss:  3835.258576194504\n",
      "Finished training iteration:  12  with val loss:  1560.1275837222927  train loss:  3842.5903682224907\n",
      "Finished training iteration:  13  with val loss:  1500.0779948210536  train loss:  3842.4073828476457\n",
      "Finished training iteration:  14  with val loss:  1499.5468760019805  train loss:  3810.8414224027233\n",
      "Finished training iteration:  15  with val loss:  1428.138795550929  train loss:  3811.3704647179898\n",
      "Finished training iteration:  16  with val loss:  1429.4341217449746  train loss:  3884.122474280705\n",
      "Finished training iteration:  17  with val loss:  1354.2332411634566  train loss:  3882.6536124168456\n",
      "Finished training iteration:  18  with val loss:  1657.876307767252  train loss:  3824.9524453066037\n",
      "Finished training iteration:  19  with val loss:  1482.4902269985253  train loss:  3825.0204373159627\n",
      "Finished training iteration:  20  with val loss:  1423.1909714976869  train loss:  3782.1367690945594\n",
      "Finished training iteration:  21  with val loss:  1719.9624924798227  train loss:  3759.507558424286\n",
      "Finished training iteration:  22  with val loss:  1565.4186492241606  train loss:  3893.1228134221037\n",
      "Finished training iteration:  23  with val loss:  1458.2970995894614  train loss:  3781.637415422585\n",
      "Finished training iteration:  24  with val loss:  1415.2003548993928  train loss:  3878.5917531139307\n",
      "Finished training iteration:  25  with val loss:  1669.023383210519  train loss:  3856.6655196514166\n",
      "Finished training iteration:  26  with val loss:  1423.457908064743  train loss:  3812.93479465599\n",
      "Finished training iteration:  27  with val loss:  1540.992319144659  train loss:  3764.193718903893\n",
      "Finished training iteration:  28  with val loss:  1467.539790069702  train loss:  3778.7806780034284\n",
      "Finished training iteration:  29  with val loss:  1499.0679608568814  train loss:  3765.915208752888\n",
      "Finished training iteration:  30  with val loss:  1497.2085101369064  train loss:  3760.8569482116095\n",
      "Finished training iteration:  31  with val loss:  1377.990527796849  train loss:  3729.8571267618763\n",
      "Finished training iteration:  32  with val loss:  1408.806377381921  train loss:  3765.3190257945225\n",
      "Finished training iteration:  33  with val loss:  1482.6833619975641  train loss:  3746.2912238549\n",
      "Finished training iteration:  34  with val loss:  1488.2900869136993  train loss:  3751.2076140692116\n",
      "Finished training iteration:  35  with val loss:  1596.295207239542  train loss:  3748.035613239852\n",
      "Finished training iteration:  36  with val loss:  1521.3800726162879  train loss:  3712.1876371985772\n",
      "Finished training iteration:  37  with val loss:  1486.4771552813215  train loss:  3719.0381069708515\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 385  # The number of expected features in the input x\n",
    "hidden_size = 512  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, target_train, target_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e7710f-b871-4353-a5e2-a557535d5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.6366768829624394\n",
      "f1:  0.7722057032078907\n",
      "f1 inverse:  0.4388894266447262\n",
      "Precision:  0.7875927670120171\n",
      "Precision inverse:  0.4187381674336991\n",
      "Recall:  0.7574083471982219\n",
      "Recall inverse:  0.46107825025846144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6366768829624394,\n",
       " 0.7722057032078907,\n",
       " 0.4388894266447262,\n",
       " 0.7875927670120171,\n",
       " 0.4187381674336991,\n",
       " 0.7574083471982219,\n",
       " 0.46107825025846144)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tt = torch.load(\"../../Model/LSTM/LSTM_TT_h\" + str(hidden_size) + \"_l\" + str(num_layers) + \"_st.LSTM\")\n",
    "eval_model(model_tt, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb34416-0094-4a5c-a816-3978bdbb0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../../Model/LSTM/LSTM_TT_h\" + str(hidden_size) + \"_l\" + str(num_layers) + \"_st.LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d53457-7dc7-4dcd-92ee-fa9c43c0c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c020c0-123c-41ac-8aaa-f0cf794fda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  1039.2492868237623  train loss:  1973.6620985799589\n",
      "Finished training iteration:  1  with val loss:  730.9840549168484  train loss:  1968.1232016268439\n",
      "Finished training iteration:  2  with val loss:  1053.200834804691  train loss:  1922.908640485293\n",
      "Finished training iteration:  3  with val loss:  817.3597401400724  train loss:  1991.4972430789105\n",
      "Finished training iteration:  4  with val loss:  772.0366282027886  train loss:  1982.9249040802117\n",
      "Finished training iteration:  5  with val loss:  752.7597583220604  train loss:  1953.9207156335165\n",
      "Finished training iteration:  6  with val loss:  644.1411491213101  train loss:  1942.0572116555575\n",
      "Finished training iteration:  7  with val loss:  623.1171479989646  train loss:  1919.0061196916886\n",
      "Finished training iteration:  8  with val loss:  736.5424007116064  train loss:  1903.243047325431\n",
      "Finished training iteration:  9  with val loss:  918.3865771184614  train loss:  1915.2791136717524\n",
      "Finished training iteration:  10  with val loss:  922.5058346920772  train loss:  1968.3128741501323\n",
      "Finished training iteration:  11  with val loss:  862.2844391662845  train loss:  1938.3867018241044\n",
      "Finished training iteration:  12  with val loss:  771.7327106954226  train loss:  1908.2170043736835\n",
      "Finished training iteration:  13  with val loss:  677.4026688270746  train loss:  1954.8381472111264\n",
      "Finished training iteration:  14  with val loss:  604.5380515371085  train loss:  1909.620216976604\n",
      "Finished training iteration:  15  with val loss:  759.0310092048729  train loss:  1893.822553061929\n",
      "Finished training iteration:  16  with val loss:  794.9368709853138  train loss:  1917.8147183312155\n",
      "Finished training iteration:  17  with val loss:  790.2855538255042  train loss:  1921.4131652405376\n",
      "Finished training iteration:  18  with val loss:  690.3257405481974  train loss:  1913.0974904629024\n",
      "Finished training iteration:  19  with val loss:  721.0475682991114  train loss:  1893.1799784251764\n",
      "Finished training iteration:  20  with val loss:  693.402355393572  train loss:  1888.6445627123908\n",
      "Finished training iteration:  21  with val loss:  643.8819742698586  train loss:  1899.4150944930363\n",
      "Finished training iteration:  22  with val loss:  667.3710618135126  train loss:  1885.796957234982\n",
      "Finished training iteration:  23  with val loss:  787.6990248786842  train loss:  1903.9633993313523\n",
      "Finished training iteration:  24  with val loss:  650.7655965238481  train loss:  1914.6905665459076\n",
      "Finished training iteration:  25  with val loss:  703.815846056374  train loss:  1883.83762219502\n",
      "Finished training iteration:  26  with val loss:  759.1122036730418  train loss:  1932.5978409360919\n",
      "Finished training iteration:  27  with val loss:  775.1150163329181  train loss:  1919.373179664268\n",
      "Finished training iteration:  28  with val loss:  803.4155761804823  train loss:  1889.6344499498277\n",
      "Finished training iteration:  29  with val loss:  727.7701884573063  train loss:  1896.0815888088225\n",
      "Finished training iteration:  30  with val loss:  889.4549829026131  train loss:  1900.2693243244\n",
      "Finished training iteration:  31  with val loss:  635.0351661567926  train loss:  1993.3616287718717\n",
      "Finished training iteration:  32  with val loss:  772.0589522149005  train loss:  1930.3195970366237\n",
      "Finished training iteration:  33  with val loss:  803.6476269883417  train loss:  1922.4504518505025\n",
      "Finished training iteration:  34  with val loss:  651.5156442361047  train loss:  1922.736818748235\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 385  # The number of expected features in the input x\n",
    "hidden_size = 512  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9355342d-dcad-4d0a-8412-1723098d1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.5778218223090072\n",
      "f1:  0.6724835690242856\n",
      "f1 inverse:  0.37357791692389103\n",
      "Precision:  0.7507761269680362\n",
      "Precision inverse:  0.31145649518121354\n",
      "Recall:  0.6089780238840103\n",
      "Recall inverse:  0.46665423792010574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5778218223090072,\n",
       " 0.6724835690242856,\n",
       " 0.37357791692389103,\n",
       " 0.7507761269680362,\n",
       " 0.31145649518121354,\n",
       " 0.6089780238840103,\n",
       " 0.46665423792010574)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e54b41-7ffc-4bd3-8384-dd6574f1e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.7069402210516883\n",
      "f1:  0.8501752076700048\n",
      "f1 inverse:  0.5089488719174489\n",
      "Precision:  0.8446313706086096\n",
      "Precision inverse:  0.5201382268827455\n",
      "Recall:  0.8557923008057297\n",
      "Recall inverse:  0.4982307955712818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7069402210516883,\n",
       " 0.8501752076700048,\n",
       " 0.5089488719174489,\n",
       " 0.8446313706086096,\n",
       " 0.5201382268827455,\n",
       " 0.8557923008057297,\n",
       " 0.4982307955712818)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
