{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de7c8c-01be-4e81-b457-be18d22d0981",
   "metadata": {},
   "source": [
    "# Manual single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee86884e-827e-45a0-97c8-456d20810ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.trainer import CaseDataSet\n",
    "from src.model import DLModels\n",
    "from src.trainer import Trainer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import importlib.util\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82498c5f-3882-47c6-916c-f17bd46a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_set, torch_device, device_package, decision_boundary=0.5,\n",
    "               weighted=True, print_res=False):\n",
    "    model.flatten()\n",
    "    res, ref, num = evaluate_model(model, test_set, torch_device, device_package)\n",
    "    res_prob = np.squeeze(torch.concat(res).numpy())\n",
    "    res_class = copy.copy(res_prob)\n",
    "    res_class[res_class < decision_boundary] = 0\n",
    "    res_class[res_class >= decision_boundary] = 1\n",
    "    ref_class = np.squeeze(torch.concat(ref).numpy()).astype(int)\n",
    "    roc_auc = roc_auc_score(ref_class, res_prob)\n",
    "    f1 = f1_score(ref_class, res_class)\n",
    "    f1_inverse = f1_score(1-ref_class, 1-res_class)\n",
    "    precision = precision_score(ref_class, res_class)\n",
    "    precision_inverse = precision_score(1-ref_class, 1-res_class)\n",
    "    recall = recall_score(ref_class, res_class)\n",
    "    recall_inverse = recall_score(1-ref_class, 1-res_class)\n",
    "    if print_res:\n",
    "        print(\"roc_auc: \", roc_auc)\n",
    "        print(\"f1: \", f1)\n",
    "        print(\"f1 inverse: \", f1_inverse)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Precision inverse: \", precision_inverse)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"Recall inverse: \", recall_inverse)\n",
    "\n",
    "    if weighted:\n",
    "        \n",
    "    else:\n",
    "        return roc_auc, f1, f1_inverse, precision, precision_inverse, recall, recall_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe758624-3ebf-4474-a222-642be5f150d3",
   "metadata": {},
   "source": [
    "## Random split w2v with time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0310d1-8d86-4a9d-8fa8-b9ba2438c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test_random_all\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6010546-eb97-4b48-a3a4-5178b818be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  996.6249177235654  train loss:  2660.7734584003942\n",
      "Finished training iteration:  1  with val loss:  1196.3048524362528  train loss:  2589.2944925410757\n",
      "Finished training iteration:  2  with val loss:  855.5405496474328  train loss:  2562.5796381589416\n",
      "Finished training iteration:  3  with val loss:  781.9919834990015  train loss:  2532.4193045895504\n",
      "Finished training iteration:  4  with val loss:  823.1721990579153  train loss:  2592.4140822618606\n",
      "Finished training iteration:  5  with val loss:  711.3471627569812  train loss:  2525.1250427588975\n",
      "Finished training iteration:  6  with val loss:  784.2330404241188  train loss:  2521.490577040124\n",
      "Finished training iteration:  7  with val loss:  700.9396956267996  train loss:  2518.2232184554523\n",
      "Finished training iteration:  8  with val loss:  695.8953017310303  train loss:  2496.858220715114\n",
      "Finished training iteration:  9  with val loss:  798.6070163065406  train loss:  2499.077705444793\n",
      "Finished training iteration:  10  with val loss:  771.9426358102133  train loss:  2534.8377576765874\n",
      "Finished training iteration:  11  with val loss:  940.5054510251233  train loss:  2557.881881320582\n",
      "Finished training iteration:  12  with val loss:  956.9672907939845  train loss:  2529.6885284238833\n",
      "Finished training iteration:  13  with val loss:  719.101321675683  train loss:  2558.9434581823753\n",
      "Finished training iteration:  14  with val loss:  755.7817586462152  train loss:  2514.2330697910206\n",
      "Finished training iteration:  15  with val loss:  705.7748758769479  train loss:  2526.0861783145715\n",
      "Finished training iteration:  16  with val loss:  661.1522509890392  train loss:  2511.521780533366\n",
      "Finished training iteration:  17  with val loss:  664.4067989390788  train loss:  2546.4340051570352\n",
      "Finished training iteration:  18  with val loss:  684.5039074747755  train loss:  2506.5514828792207\n",
      "Finished training iteration:  19  with val loss:  653.1601764973265  train loss:  2503.50588359288\n",
      "Finished training iteration:  20  with val loss:  698.3213613756637  train loss:  2486.7416958982626\n",
      "Finished training iteration:  21  with val loss:  966.2308303943373  train loss:  2498.0520028183346\n",
      "Finished training iteration:  22  with val loss:  697.6178610986657  train loss:  2539.6841680789657\n",
      "Finished training iteration:  23  with val loss:  643.1832442339795  train loss:  2531.268445694052\n",
      "Finished training iteration:  24  with val loss:  751.5890879863867  train loss:  2504.642936320522\n",
      "Finished training iteration:  25  with val loss:  659.1855821488125  train loss:  2509.7119861219962\n",
      "Finished training iteration:  26  with val loss:  660.7257111510133  train loss:  2498.8352666968935\n",
      "Finished training iteration:  27  with val loss:  673.4081723541907  train loss:  2502.2118139792165\n",
      "Finished training iteration:  28  with val loss:  673.372263692201  train loss:  2500.8672518494704\n",
      "Finished training iteration:  29  with val loss:  652.0835073140432  train loss:  2511.4476673829286\n",
      "Finished training iteration:  30  with val loss:  815.6640741253412  train loss:  2489.0128843951416\n",
      "Finished training iteration:  31  with val loss:  647.9904101167289  train loss:  2547.225403600917\n",
      "Finished training iteration:  32  with val loss:  645.1561118764772  train loss:  2535.4907270739222\n",
      "Finished training iteration:  33  with val loss:  644.6977591513645  train loss:  2488.293056870061\n",
      "Finished training iteration:  34  with val loss:  689.4964513231448  train loss:  2496.074039722024\n",
      "Finished training iteration:  35  with val loss:  677.3876082888288  train loss:  2503.901139551737\n",
      "Finished training iteration:  36  with val loss:  688.206190479529  train loss:  2528.3358818174875\n",
      "Finished training iteration:  37  with val loss:  642.8069155362065  train loss:  2485.2290803453498\n",
      "Finished training iteration:  38  with val loss:  721.3073677828228  train loss:  2530.7371799961884\n",
      "Finished training iteration:  39  with val loss:  666.1833867429152  train loss:  2512.6098425965915\n",
      "Finished training iteration:  40  with val loss:  620.0768082495354  train loss:  2511.6292986055596\n",
      "Finished training iteration:  41  with val loss:  625.1317846180839  train loss:  2489.3376842911043\n",
      "Finished training iteration:  42  with val loss:  685.363139593676  train loss:  2471.2820090503674\n",
      "Finished training iteration:  43  with val loss:  645.5290754279947  train loss:  2490.109812273905\n",
      "Finished training iteration:  44  with val loss:  642.1508758451655  train loss:  2486.4411959586573\n",
      "Finished training iteration:  45  with val loss:  660.6840951715428  train loss:  2487.087081545491\n",
      "Finished training iteration:  46  with val loss:  667.3296229256028  train loss:  2499.186855121334\n",
      "Finished training iteration:  47  with val loss:  642.357321680425  train loss:  2482.044228852515\n",
      "Finished training iteration:  48  with val loss:  625.2109043552204  train loss:  2480.624019843124\n",
      "Finished training iteration:  49  with val loss:  627.2281849416925  train loss:  2465.619775702411\n",
      "Finished training iteration:  50  with val loss:  633.843495158078  train loss:  2466.779654722464\n",
      "Finished training iteration:  51  with val loss:  642.91297762919  train loss:  2465.4401478537943\n",
      "Finished training iteration:  52  with val loss:  662.0562375384698  train loss:  2472.8804436466876\n",
      "Finished training iteration:  53  with val loss:  681.7165181980247  train loss:  2475.652344720467\n",
      "Finished training iteration:  54  with val loss:  634.2056720981907  train loss:  2473.1654636034973\n",
      "Finished training iteration:  55  with val loss:  620.3952883539118  train loss:  2463.491493127075\n",
      "Finished training iteration:  56  with val loss:  689.9076118461505  train loss:  2463.0352393954363\n",
      "Finished training iteration:  57  with val loss:  632.8577385973269  train loss:  2471.391818131532\n",
      "Finished training iteration:  58  with val loss:  633.5480258730413  train loss:  2483.969866567697\n",
      "Finished training iteration:  59  with val loss:  646.7535662611803  train loss:  2471.220648992159\n",
      "Finished training iteration:  60  with val loss:  675.1609553757918  train loss:  2471.720353752703\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 51  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d721b0-0ae2-4b8c-b2fa-5b1c3c37a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.788782648557422\n",
      "f1:  0.8300555396120669\n",
      "f1 inverse:  0.5439637599093998\n",
      "Precision:  0.8911160245025191\n",
      "Precision inverse:  0.4594780745389148\n",
      "Recall:  0.7768263397371082\n",
      "Recall inverse:  0.66651865008881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.788782648557422,\n",
       " 0.8300555396120669,\n",
       " 0.5439637599093998,\n",
       " 0.8911160245025191,\n",
       " 0.4594780745389148,\n",
       " 0.7768263397371082,\n",
       " 0.66651865008881)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a777e322-3716-4d11-b6ff-7e990c7a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.4230700553058763\n",
      "f1:  0.6871052202220479\n",
      "f1 inverse:  0.26430811386816866\n",
      "Precision:  0.6811446117192795\n",
      "Precision inverse:  0.2698606120139761\n",
      "Recall:  0.6931710707138267\n",
      "Recall inverse:  0.25897949929595954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4230700553058763,\n",
       " 0.6871052202220479,\n",
       " 0.26430811386816866,\n",
       " 0.6811446117192795,\n",
       " 0.2698606120139761,\n",
       " 0.6931710707138267,\n",
       " 0.25897949929595954)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b36b9-2645-40dc-8d02-ef542095d7ba",
   "metadata": {},
   "source": [
    "## Temporal split w2v with time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b0ade2-6ce0-4265-9ad3-9010c5e98d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_w2v\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84466f3-8750-482d-99f4-87a37685dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  770.7401489121513  train loss:  2111.58595620398\n",
      "Finished training iteration:  1  with val loss:  639.5801284197825  train loss:  1977.5015798619638\n",
      "Finished training iteration:  2  with val loss:  704.881448928058  train loss:  1964.4997446317514\n",
      "Finished training iteration:  3  with val loss:  668.2876379987465  train loss:  1975.0989536021889\n",
      "Finished training iteration:  4  with val loss:  614.1776659076711  train loss:  1978.8005905498533\n",
      "Finished training iteration:  5  with val loss:  795.8914965395888  train loss:  1952.002751499723\n",
      "Finished training iteration:  6  with val loss:  980.0886866191778  train loss:  1976.1910546374884\n",
      "Finished training iteration:  7  with val loss:  608.0032991130973  train loss:  2033.2781942557572\n",
      "Finished training iteration:  8  with val loss:  906.8084920839253  train loss:  1987.85975196129\n",
      "Finished training iteration:  9  with val loss:  669.5728545660515  train loss:  1981.9269394000075\n",
      "Finished training iteration:  10  with val loss:  678.8336844170116  train loss:  1981.2621113752766\n",
      "Finished training iteration:  11  with val loss:  604.8302387499945  train loss:  1984.1643644284716\n",
      "Finished training iteration:  12  with val loss:  847.0602758211553  train loss:  1948.2640666530451\n",
      "Finished training iteration:  13  with val loss:  651.9946966334588  train loss:  2009.4485381616007\n",
      "Finished training iteration:  14  with val loss:  622.8119700178228  train loss:  1989.5172907715519\n",
      "Finished training iteration:  15  with val loss:  889.9592930291369  train loss:  1949.6089641984058\n",
      "Finished training iteration:  16  with val loss:  740.100197328168  train loss:  1992.5161107114805\n",
      "Finished training iteration:  17  with val loss:  627.182992335422  train loss:  2004.922910037455\n",
      "Finished training iteration:  18  with val loss:  658.6224689908297  train loss:  1935.2599430661737\n",
      "Finished training iteration:  19  with val loss:  609.5564629040614  train loss:  1944.1946332572759\n",
      "Finished training iteration:  20  with val loss:  676.4145884720897  train loss:  1977.992008756913\n",
      "Finished training iteration:  21  with val loss:  674.7414857428332  train loss:  1957.4046189941673\n",
      "Finished training iteration:  22  with val loss:  639.8238698091476  train loss:  1940.0178965066239\n",
      "Finished training iteration:  23  with val loss:  629.7049019156059  train loss:  1970.5547724236471\n",
      "Finished training iteration:  24  with val loss:  659.5195500517867  train loss:  1938.9355086436642\n",
      "Finished training iteration:  25  with val loss:  563.8310089893132  train loss:  1965.3286099910365\n",
      "Finished training iteration:  26  with val loss:  556.2820939732803  train loss:  1951.6325317248438\n",
      "Finished training iteration:  27  with val loss:  586.1465136629738  train loss:  1937.1804376183995\n",
      "Finished training iteration:  28  with val loss:  614.9876613451455  train loss:  1932.6895211105978\n",
      "Finished training iteration:  29  with val loss:  570.7431729992064  train loss:  1961.18765172325\n",
      "Finished training iteration:  30  with val loss:  558.8593400351076  train loss:  1939.3319570924175\n",
      "Finished training iteration:  31  with val loss:  890.787908375165  train loss:  1943.3359674776113\n",
      "Finished training iteration:  32  with val loss:  580.9394407758762  train loss:  1985.800981533283\n",
      "Finished training iteration:  33  with val loss:  564.9902608475162  train loss:  1950.917817679336\n",
      "Finished training iteration:  34  with val loss:  635.2037524083211  train loss:  1916.6317042021506\n",
      "Finished training iteration:  35  with val loss:  608.0763116567258  train loss:  1921.4758986603022\n",
      "Finished training iteration:  36  with val loss:  638.9148062596626  train loss:  1922.1789993983057\n",
      "Finished training iteration:  37  with val loss:  691.0253358907896  train loss:  1923.6465590954922\n",
      "Finished training iteration:  38  with val loss:  641.3131409252311  train loss:  1941.9421891953116\n",
      "Finished training iteration:  39  with val loss:  568.405855407516  train loss:  1937.3223553133826\n",
      "Finished training iteration:  40  with val loss:  571.9681149830487  train loss:  1916.9840626488394\n",
      "Finished training iteration:  41  with val loss:  587.7893753688456  train loss:  1905.3982092354559\n",
      "Finished training iteration:  42  with val loss:  590.8207718423059  train loss:  1911.6872133160978\n",
      "Finished training iteration:  43  with val loss:  598.3714036519133  train loss:  1914.9654169372927\n",
      "Finished training iteration:  44  with val loss:  568.9011018833609  train loss:  1936.0861744196377\n",
      "Finished training iteration:  45  with val loss:  577.730751602623  train loss:  1910.1705729823768\n",
      "Finished training iteration:  46  with val loss:  554.101978644166  train loss:  1910.9070661310805\n",
      "Finished training iteration:  47  with val loss:  539.8677015169606  train loss:  1903.5947679978303\n",
      "Finished training iteration:  48  with val loss:  561.009098795834  train loss:  1897.0607900071977\n",
      "Finished training iteration:  49  with val loss:  580.3410907844551  train loss:  1895.8112682965927\n",
      "Finished training iteration:  50  with val loss:  563.0158816331747  train loss:  1905.3606489963845\n",
      "Finished training iteration:  51  with val loss:  558.0511832560867  train loss:  1900.5159801691066\n",
      "Finished training iteration:  52  with val loss:  560.4701990978974  train loss:  1897.066545558485\n",
      "Finished training iteration:  53  with val loss:  581.2955946157014  train loss:  1894.3918287192855\n",
      "Finished training iteration:  54  with val loss:  581.8202028929164  train loss:  1899.3718449486364\n",
      "Finished training iteration:  55  with val loss:  569.4383767363189  train loss:  1890.7262068354985\n",
      "Finished training iteration:  56  with val loss:  548.5496212956093  train loss:  1893.8758492618322\n",
      "Finished training iteration:  57  with val loss:  553.3863975009137  train loss:  1891.7121396578639\n",
      "Finished training iteration:  58  with val loss:  548.2869586897646  train loss:  1900.3186621531008\n",
      "Finished training iteration:  59  with val loss:  651.1856906177815  train loss:  1896.5759690161642\n",
      "Finished training iteration:  60  with val loss:  749.981760167649  train loss:  1905.7435979311847\n",
      "Finished training iteration:  61  with val loss:  602.1556236275349  train loss:  1936.0361958949704\n",
      "Finished training iteration:  62  with val loss:  588.6287202493295  train loss:  1906.3341134927098\n",
      "Finished training iteration:  63  with val loss:  636.6731042919832  train loss:  1895.803415326068\n",
      "Finished training iteration:  64  with val loss:  596.5694699669476  train loss:  1892.5602641549212\n",
      "Finished training iteration:  65  with val loss:  581.8191669793714  train loss:  1900.5963525199338\n",
      "Finished training iteration:  66  with val loss:  617.7484421299928  train loss:  1904.658136256158\n",
      "Finished training iteration:  67  with val loss:  668.2574562989437  train loss:  1895.1077236214353\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 51  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7eeb46-1b50-4fef-ae30-05549fad4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.7438235988684694\n",
      "f1:  0.8171005259090309\n",
      "f1 inverse:  0.5225136558240243\n",
      "Precision:  0.8626124333359866\n",
      "Precision inverse:  0.4592560553633218\n",
      "Recall:  0.7761504028648165\n",
      "Recall inverse:  0.6059810523912795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7438235988684694,\n",
       " 0.8171005259090309,\n",
       " 0.5225136558240243,\n",
       " 0.8626124333359866,\n",
       " 0.4592560553633218,\n",
       " 0.7761504028648165,\n",
       " 0.6059810523912795)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0945d41d-2e7b-4e3f-b772-03d5262eec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.644895825098191\n",
      "f1:  0.641076460933387\n",
      "f1 inverse:  0.45782850727417324\n",
      "Precision:  0.8065927001042286\n",
      "Precision inverse:  0.34949514788462055\n",
      "Recall:  0.531923504358607\n",
      "Recall inverse:  0.6634916868633798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.644895825098191,\n",
       " 0.641076460933387,\n",
       " 0.45782850727417324,\n",
       " 0.8065927001042286,\n",
       " 0.34949514788462055,\n",
       " 0.531923504358607,\n",
       " 0.6634916868633798)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83d3fbb3-a0ca-4b8d-840c-f5897ebe1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50ea6508-3883-426a-90b3-4068d5360527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  1988.9160514570456  train loss:  4317.641623615202\n",
      "Finished training iteration:  1  with val loss:  1682.0884687426717  train loss:  4183.40916337637\n",
      "Finished training iteration:  2  with val loss:  1820.4668025274987  train loss:  4113.818625999076\n",
      "Finished training iteration:  3  with val loss:  1666.8854319424315  train loss:  4013.5959644560708\n",
      "Finished training iteration:  4  with val loss:  1683.3796946949776  train loss:  3947.536043122505\n",
      "Finished training iteration:  5  with val loss:  1641.8601277235052  train loss:  3958.130681605511\n",
      "Finished training iteration:  6  with val loss:  1682.2979228832246  train loss:  3898.806011466092\n",
      "Finished training iteration:  7  with val loss:  1495.3503464992973  train loss:  3921.5271092744792\n",
      "Finished training iteration:  8  with val loss:  1443.6140393632006  train loss:  3919.21339096277\n",
      "Finished training iteration:  9  with val loss:  1450.4088309332203  train loss:  3852.6280342132545\n",
      "Finished training iteration:  10  with val loss:  1422.5088776529483  train loss:  3825.4753223426883\n",
      "Finished training iteration:  11  with val loss:  1391.6212728829937  train loss:  3835.258576194504\n",
      "Finished training iteration:  12  with val loss:  1560.1275837222927  train loss:  3842.5903682224907\n",
      "Finished training iteration:  13  with val loss:  1500.0779948210536  train loss:  3842.4073828476457\n",
      "Finished training iteration:  14  with val loss:  1499.5468760019805  train loss:  3810.8414224027233\n",
      "Finished training iteration:  15  with val loss:  1428.138795550929  train loss:  3811.3704647179898\n",
      "Finished training iteration:  16  with val loss:  1429.4341217449746  train loss:  3884.122474280705\n",
      "Finished training iteration:  17  with val loss:  1354.2332411634566  train loss:  3882.6536124168456\n",
      "Finished training iteration:  18  with val loss:  1657.876307767252  train loss:  3824.9524453066037\n",
      "Finished training iteration:  19  with val loss:  1482.4902269985253  train loss:  3825.0204373159627\n",
      "Finished training iteration:  20  with val loss:  1423.1909714976869  train loss:  3782.1367690945594\n",
      "Finished training iteration:  21  with val loss:  1719.9624924798227  train loss:  3759.507558424286\n",
      "Finished training iteration:  22  with val loss:  1565.4186492241606  train loss:  3893.1228134221037\n",
      "Finished training iteration:  23  with val loss:  1458.2970995894614  train loss:  3781.637415422585\n",
      "Finished training iteration:  24  with val loss:  1415.2003548993928  train loss:  3878.5917531139307\n",
      "Finished training iteration:  25  with val loss:  1669.023383210519  train loss:  3856.6655196514166\n",
      "Finished training iteration:  26  with val loss:  1423.457908064743  train loss:  3812.93479465599\n",
      "Finished training iteration:  27  with val loss:  1540.992319144659  train loss:  3764.193718903893\n",
      "Finished training iteration:  28  with val loss:  1467.539790069702  train loss:  3778.7806780034284\n",
      "Finished training iteration:  29  with val loss:  1499.0679608568814  train loss:  3765.915208752888\n",
      "Finished training iteration:  30  with val loss:  1497.2085101369064  train loss:  3760.8569482116095\n",
      "Finished training iteration:  31  with val loss:  1377.990527796849  train loss:  3729.8571267618763\n",
      "Finished training iteration:  32  with val loss:  1408.806377381921  train loss:  3765.3190257945225\n",
      "Finished training iteration:  33  with val loss:  1482.6833619975641  train loss:  3746.2912238549\n",
      "Finished training iteration:  34  with val loss:  1488.2900869136993  train loss:  3751.2076140692116\n",
      "Finished training iteration:  35  with val loss:  1596.295207239542  train loss:  3748.035613239852\n",
      "Finished training iteration:  36  with val loss:  1521.3800726162879  train loss:  3712.1876371985772\n",
      "Finished training iteration:  37  with val loss:  1486.4771552813215  train loss:  3719.0381069708515\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 385  # The number of expected features in the input x\n",
    "hidden_size = 512  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, target_train, target_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e7710f-b871-4353-a5e2-a557535d5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.6366768829624394\n",
      "f1:  0.7722057032078907\n",
      "f1 inverse:  0.4388894266447262\n",
      "Precision:  0.7875927670120171\n",
      "Precision inverse:  0.4187381674336991\n",
      "Recall:  0.7574083471982219\n",
      "Recall inverse:  0.46107825025846144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6366768829624394,\n",
       " 0.7722057032078907,\n",
       " 0.4388894266447262,\n",
       " 0.7875927670120171,\n",
       " 0.4187381674336991,\n",
       " 0.7574083471982219,\n",
       " 0.46107825025846144)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tt = torch.load(\"../../Model/LSTM/LSTM_TT_h\" + str(hidden_size) + \"_l\" + str(num_layers) + \"_st.LSTM\")\n",
    "eval_model(model_tt, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb34416-0094-4a5c-a816-3978bdbb0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../../Model/LSTM/LSTM_TT_h\" + str(hidden_size) + \"_l\" + str(num_layers) + \"_st.LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d53457-7dc7-4dcd-92ee-fa9c43c0c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_st\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c020c0-123c-41ac-8aaa-f0cf794fda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  1039.2492868237623  train loss:  1973.6620985799589\n",
      "Finished training iteration:  1  with val loss:  730.9840549168484  train loss:  1968.1232016268439\n",
      "Finished training iteration:  2  with val loss:  1053.200834804691  train loss:  1922.908640485293\n",
      "Finished training iteration:  3  with val loss:  817.3597401400724  train loss:  1991.4972430789105\n",
      "Finished training iteration:  4  with val loss:  772.0366282027886  train loss:  1982.9249040802117\n",
      "Finished training iteration:  5  with val loss:  752.7597583220604  train loss:  1953.9207156335165\n",
      "Finished training iteration:  6  with val loss:  644.1411491213101  train loss:  1942.0572116555575\n",
      "Finished training iteration:  7  with val loss:  623.1171479989646  train loss:  1919.0061196916886\n",
      "Finished training iteration:  8  with val loss:  736.5424007116064  train loss:  1903.243047325431\n",
      "Finished training iteration:  9  with val loss:  918.3865771184614  train loss:  1915.2791136717524\n",
      "Finished training iteration:  10  with val loss:  922.5058346920772  train loss:  1968.3128741501323\n",
      "Finished training iteration:  11  with val loss:  862.2844391662845  train loss:  1938.3867018241044\n",
      "Finished training iteration:  12  with val loss:  771.7327106954226  train loss:  1908.2170043736835\n",
      "Finished training iteration:  13  with val loss:  677.4026688270746  train loss:  1954.8381472111264\n",
      "Finished training iteration:  14  with val loss:  604.5380515371085  train loss:  1909.620216976604\n",
      "Finished training iteration:  15  with val loss:  759.0310092048729  train loss:  1893.822553061929\n",
      "Finished training iteration:  16  with val loss:  794.9368709853138  train loss:  1917.8147183312155\n",
      "Finished training iteration:  17  with val loss:  790.2855538255042  train loss:  1921.4131652405376\n",
      "Finished training iteration:  18  with val loss:  690.3257405481974  train loss:  1913.0974904629024\n",
      "Finished training iteration:  19  with val loss:  721.0475682991114  train loss:  1893.1799784251764\n",
      "Finished training iteration:  20  with val loss:  693.402355393572  train loss:  1888.6445627123908\n",
      "Finished training iteration:  21  with val loss:  643.8819742698586  train loss:  1899.4150944930363\n",
      "Finished training iteration:  22  with val loss:  667.3710618135126  train loss:  1885.796957234982\n",
      "Finished training iteration:  23  with val loss:  787.6990248786842  train loss:  1903.9633993313523\n",
      "Finished training iteration:  24  with val loss:  650.7655965238481  train loss:  1914.6905665459076\n",
      "Finished training iteration:  25  with val loss:  703.815846056374  train loss:  1883.83762219502\n",
      "Finished training iteration:  26  with val loss:  759.1122036730418  train loss:  1932.5978409360919\n",
      "Finished training iteration:  27  with val loss:  775.1150163329181  train loss:  1919.373179664268\n",
      "Finished training iteration:  28  with val loss:  803.4155761804823  train loss:  1889.6344499498277\n",
      "Finished training iteration:  29  with val loss:  727.7701884573063  train loss:  1896.0815888088225\n",
      "Finished training iteration:  30  with val loss:  889.4549829026131  train loss:  1900.2693243244\n",
      "Finished training iteration:  31  with val loss:  635.0351661567926  train loss:  1993.3616287718717\n",
      "Finished training iteration:  32  with val loss:  772.0589522149005  train loss:  1930.3195970366237\n",
      "Finished training iteration:  33  with val loss:  803.6476269883417  train loss:  1922.4504518505025\n",
      "Finished training iteration:  34  with val loss:  651.5156442361047  train loss:  1922.736818748235\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 385  # The number of expected features in the input x\n",
    "hidden_size = 512  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9355342d-dcad-4d0a-8412-1723098d1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.5778218223090072\n",
      "f1:  0.6724835690242856\n",
      "f1 inverse:  0.37357791692389103\n",
      "Precision:  0.7507761269680362\n",
      "Precision inverse:  0.31145649518121354\n",
      "Recall:  0.6089780238840103\n",
      "Recall inverse:  0.46665423792010574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5778218223090072,\n",
       " 0.6724835690242856,\n",
       " 0.37357791692389103,\n",
       " 0.7507761269680362,\n",
       " 0.31145649518121354,\n",
       " 0.6089780238840103,\n",
       " 0.46665423792010574)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e54b41-7ffc-4bd3-8384-dd6574f1e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.7069402210516883\n",
      "f1:  0.8501752076700048\n",
      "f1 inverse:  0.5089488719174489\n",
      "Precision:  0.8446313706086096\n",
      "Precision inverse:  0.5201382268827455\n",
      "Recall:  0.8557923008057297\n",
      "Recall inverse:  0.4982307955712818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7069402210516883,\n",
       " 0.8501752076700048,\n",
       " 0.5089488719174489,\n",
       " 0.8446313706086096,\n",
       " 0.5201382268827455,\n",
       " 0.8557923008057297,\n",
       " 0.4982307955712818)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300b1f0-2f6b-4956-af1d-c187a264ea66",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073b9511-31a8-4c2c-8713-41226273d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)\n",
    "target_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9528bf28-f781-45ff-8901-93d2d7768e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  2170.1126981360208  train loss:  4352.60548912944\n",
      "Finished training iteration:  1  with val loss:  1810.327537026345  train loss:  4428.924114136297\n",
      "Finished training iteration:  2  with val loss:  1778.3200395160427  train loss:  4340.564020028704\n",
      "Finished training iteration:  3  with val loss:  1893.6346498465555  train loss:  4355.280440942198\n",
      "Finished training iteration:  4  with val loss:  1539.5768292573628  train loss:  4387.240193306612\n",
      "Finished training iteration:  5  with val loss:  2661.6335987820103  train loss:  4314.496769521407\n",
      "Finished training iteration:  6  with val loss:  1591.3304440736624  train loss:  4425.871598136476\n",
      "Finished training iteration:  7  with val loss:  1752.8828915616607  train loss:  4347.596576816226\n",
      "Finished training iteration:  8  with val loss:  1605.7442369268829  train loss:  4283.1803461094805\n",
      "Finished training iteration:  9  with val loss:  2547.0448565741112  train loss:  4293.4640021719515\n",
      "Finished training iteration:  10  with val loss:  1706.5794756793441  train loss:  4317.142127470446\n",
      "Finished training iteration:  11  with val loss:  1310.924315896714  train loss:  4199.166130329118\n",
      "Finished training iteration:  12  with val loss:  1330.6157063481307  train loss:  4188.743545895834\n",
      "Finished training iteration:  13  with val loss:  1340.0104684463408  train loss:  4110.323324999293\n",
      "Finished training iteration:  14  with val loss:  3195.3515364149025  train loss:  4082.9149582385803\n",
      "Finished training iteration:  15  with val loss:  2193.738552662813  train loss:  4144.944789418894\n",
      "Finished training iteration:  16  with val loss:  2151.5833303417503  train loss:  4180.35329982128\n",
      "Finished training iteration:  17  with val loss:  2359.2324228958914  train loss:  4273.806570732011\n",
      "Finished training iteration:  18  with val loss:  1835.2148530869322  train loss:  4314.288726886801\n",
      "Finished training iteration:  19  with val loss:  1328.345074189739  train loss:  4140.425763815103\n",
      "Finished training iteration:  20  with val loss:  1272.4626980153282  train loss:  4107.928594631403\n",
      "Finished training iteration:  21  with val loss:  2375.767120877969  train loss:  4111.068864856152\n",
      "Finished training iteration:  22  with val loss:  1671.02029565368  train loss:  4345.56292735247\n",
      "Finished training iteration:  23  with val loss:  1640.3518115539232  train loss:  4199.659003407544\n",
      "Finished training iteration:  24  with val loss:  2178.244458936363  train loss:  4167.2655048218985\n",
      "Finished training iteration:  25  with val loss:  2230.3137910832816  train loss:  4127.274701775218\n",
      "Finished training iteration:  26  with val loss:  1568.1306425037  train loss:  4138.9592712358235\n",
      "Finished training iteration:  27  with val loss:  1798.6683304738508  train loss:  4097.870080385025\n",
      "Finished training iteration:  28  with val loss:  1370.00858069016  train loss:  4079.7295996123416\n",
      "Finished training iteration:  29  with val loss:  1595.1223480132546  train loss:  4058.9858287103607\n",
      "Finished training iteration:  30  with val loss:  1466.163829327033  train loss:  4041.2132009434667\n",
      "Finished training iteration:  31  with val loss:  1746.8616273461503  train loss:  4099.843957169751\n",
      "Finished training iteration:  32  with val loss:  1828.424811522447  train loss:  4079.905369184599\n",
      "Finished training iteration:  33  with val loss:  1637.610549523479  train loss:  4093.9389025790642\n",
      "Finished training iteration:  34  with val loss:  1843.0847841865418  train loss:  4033.619875387347\n",
      "Finished training iteration:  35  with val loss:  1467.0964144531756  train loss:  4030.0404657400923\n",
      "Finished training iteration:  36  with val loss:  1272.4178067995995  train loss:  4010.056829099415\n",
      "Finished training iteration:  37  with val loss:  2226.3572843960915  train loss:  3973.096612396596\n",
      "Finished training iteration:  38  with val loss:  1402.4573101753522  train loss:  4076.3027773073445\n",
      "Finished training iteration:  39  with val loss:  1304.6785075418381  train loss:  4009.969513498499\n",
      "Finished training iteration:  40  with val loss:  1314.1634940802678  train loss:  3989.9216037716524\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 48  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, target_train, target_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0d38a0-b086-4b3d-a13d-f285f6f7b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.6500524453131455\n",
      "f1:  0.7800741615553607\n",
      "f1 inverse:  0.3942250515932702\n",
      "Precision:  0.7711384078484048\n",
      "Precision inverse:  0.40722273408848664\n",
      "Recall:  0.7892194335416813\n",
      "Recall inverse:  0.3820314221310781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6500524453131455,\n",
       " 0.7800741615553607,\n",
       " 0.3942250515932702,\n",
       " 0.7711384078484048,\n",
       " 0.40722273408848664,\n",
       " 0.7892194335416813,\n",
       " 0.3820314221310781)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb29528-15f6-457b-aeb2-0ca3a25772a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../../Model/LSTM/LSTM_TT_h\" + str(hidden_size) + \"_l\" + str(num_layers) + \"_onehot.LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9da5c8-7941-4c1f-bcb1-97b05c4846bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_train\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)\n",
    "source_val = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_val\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)\n",
    "source_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"source\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)\n",
    "target_test = CaseDataSet.CaseDataset(split_pattern=\"641620split\", input_data=\"target\", data_version=\"_test\",\n",
    "                                       embedding_version=\"_onehot\", earliness_requirement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9860b0-ce07-474d-9721-763128a0ec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training iteration:  0  with val loss:  678.9203514885486  train loss:  2234.2490897290368\n",
      "Finished training iteration:  1  with val loss:  920.4566444249734  train loss:  2112.3043458266047\n",
      "Finished training iteration:  2  with val loss:  662.4384308643331  train loss:  2339.2357575995634\n",
      "Finished training iteration:  3  with val loss:  608.767094538989  train loss:  2131.1151220662746\n",
      "Finished training iteration:  4  with val loss:  625.6503372921335  train loss:  2071.4123074762074\n",
      "Finished training iteration:  5  with val loss:  579.6395537601221  train loss:  2063.8861011068657\n",
      "Finished training iteration:  6  with val loss:  637.8031599819273  train loss:  2029.531731900665\n",
      "Finished training iteration:  7  with val loss:  722.5074808455742  train loss:  2093.4012284026417\n",
      "Finished training iteration:  8  with val loss:  580.7969552823214  train loss:  2133.9583338149396\n",
      "Finished training iteration:  9  with val loss:  818.0411145042092  train loss:  1994.4771580714441\n",
      "Finished training iteration:  10  with val loss:  725.4810047223148  train loss:  2082.1544554366083\n",
      "Finished training iteration:  11  with val loss:  572.2385114431074  train loss:  1991.6383074928506\n",
      "Finished training iteration:  12  with val loss:  584.5889629882466  train loss:  1960.1516933273704\n",
      "Finished training iteration:  13  with val loss:  620.6210336973653  train loss:  1969.2878429784296\n",
      "Finished training iteration:  14  with val loss:  566.2497524330483  train loss:  2016.6320381867108\n",
      "Finished training iteration:  15  with val loss:  591.7631686645163  train loss:  1967.7659277525054\n",
      "Finished training iteration:  16  with val loss:  733.9800357818776  train loss:  2007.2251204280456\n",
      "Finished training iteration:  17  with val loss:  643.1125617225264  train loss:  1976.722381509519\n",
      "Finished training iteration:  18  with val loss:  774.9906444440461  train loss:  2006.8049289176004\n",
      "Finished training iteration:  19  with val loss:  625.8576689104672  train loss:  2340.282352400704\n",
      "Finished training iteration:  20  with val loss:  591.026502531125  train loss:  2033.2697572835334\n",
      "Finished training iteration:  21  with val loss:  587.491463743156  train loss:  1993.9949471061993\n",
      "Finished training iteration:  22  with val loss:  700.6824419545732  train loss:  1983.1902101550213\n",
      "Finished training iteration:  23  with val loss:  562.5519948958099  train loss:  2100.017658954096\n",
      "Finished training iteration:  24  with val loss:  592.3948069660279  train loss:  1950.480035589519\n",
      "Finished training iteration:  25  with val loss:  566.252151562843  train loss:  1991.107627726544\n",
      "Finished training iteration:  26  with val loss:  593.373360071791  train loss:  1949.3435185257122\n",
      "Finished training iteration:  27  with val loss:  559.9151813118744  train loss:  1946.628264089085\n",
      "Finished training iteration:  28  with val loss:  586.171729130719  train loss:  1943.2251107132795\n",
      "Finished training iteration:  29  with val loss:  615.8807745895275  train loss:  1960.4947147217724\n",
      "Finished training iteration:  30  with val loss:  582.5991109490692  train loss:  1969.8458465126082\n",
      "Finished training iteration:  31  with val loss:  638.7651900517286  train loss:  1948.9303557931041\n",
      "Finished training iteration:  32  with val loss:  561.3174367415568  train loss:  1987.9131087431604\n",
      "Finished training iteration:  33  with val loss:  553.003481697444  train loss:  1958.329457507632\n",
      "Finished training iteration:  34  with val loss:  555.1299150972861  train loss:  1943.4347615647755\n",
      "Finished training iteration:  35  with val loss:  686.8269780675682  train loss:  1932.9386473127436\n",
      "Finished training iteration:  36  with val loss:  549.0585461176396  train loss:  1995.33388703084\n",
      "Finished training iteration:  37  with val loss:  565.823426091843  train loss:  1934.5910426148566\n",
      "Finished training iteration:  38  with val loss:  563.4137078333068  train loss:  1932.190384314445\n",
      "Finished training iteration:  39  with val loss:  568.5851480904914  train loss:  1932.1488355070321\n",
      "Finished training iteration:  40  with val loss:  560.9469106919504  train loss:  1935.6037874422216\n",
      "Finished training iteration:  41  with val loss:  553.461437147216  train loss:  1923.468218808454\n",
      "Finished training iteration:  42  with val loss:  554.8696488706388  train loss:  1921.865103665158\n",
      "Finished training iteration:  43  with val loss:  590.0706671206774  train loss:  1919.1152004253338\n",
      "Finished training iteration:  44  with val loss:  554.2202644881893  train loss:  1921.4287606015887\n",
      "Finished training iteration:  45  with val loss:  602.1329628353609  train loss:  1912.2949191512682\n",
      "Finished training iteration:  46  with val loss:  626.6325650379066  train loss:  1919.1898204108466\n",
      "Finished training iteration:  47  with val loss:  582.5874420570623  train loss:  1955.9615989471574\n",
      "Finished training iteration:  48  with val loss:  595.1896906224  train loss:  1975.5906794085347\n",
      "Finished training iteration:  49  with val loss:  593.5581711834833  train loss:  1979.2668159437596\n",
      "Finished training iteration:  50  with val loss:  612.7749690337305  train loss:  1954.5730365251945\n",
      "Finished training iteration:  51  with val loss:  668.9751077683412  train loss:  1978.7586665776253\n",
      "Finished training iteration:  52  with val loss:  633.7599629670693  train loss:  1953.8994506812062\n",
      "Finished training iteration:  53  with val loss:  647.1638235744891  train loss:  1979.108302663832\n",
      "Finished training iteration:  54  with val loss:  651.6277456943759  train loss:  1943.1974851349673\n",
      "Finished training iteration:  55  with val loss:  653.4695724744446  train loss:  1977.1991170732279\n",
      "Finished training iteration:  56  with val loss:  613.9845407277924  train loss:  1957.1732268501169\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 48  # The number of expected features in the input x\n",
    "hidden_size = 128  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1  # For binary classification\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, train_loss, val_loss = Trainer.train_model(model, optimizer, None, None, source_train, source_val, batch_size,\n",
    "                                                  torch_device, device_package, eval_func=Trainer.prefix_weighted_loss,\n",
    "                                                  max_epoch=100, max_ob_iter=20, score_margin=1, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ecbc026-80bf-4c6b-8dda-a36580c0fdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.5109444243307237\n",
      "f1:  0.8370700333882969\n",
      "f1 inverse:  0.06385218470627052\n",
      "Precision:  0.7287456910507171\n",
      "Precision inverse:  0.43755383290267014\n",
      "Recall:  0.9832208082534095\n",
      "Recall inverse:  0.03443892683422877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5109444243307237,\n",
       " 0.8370700333882969,\n",
       " 0.06385218470627052,\n",
       " 0.7287456910507171,\n",
       " 0.43755383290267014,\n",
       " 0.9832208082534095,\n",
       " 0.03443892683422877)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, target_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d81b86-6674-4b90-9a72-b8794d64ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc:  0.7322679200988101\n",
      "f1:  0.8396849362414276\n",
      "f1 inverse:  0.521006576444372\n",
      "Precision:  0.8532978408754807\n",
      "Precision inverse:  0.4973023448848309\n",
      "Recall:  0.8264995523724261\n",
      "Recall inverse:  0.5470836662481452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7322679200988101,\n",
       " 0.8396849362414276,\n",
       " 0.521006576444372,\n",
       " 0.8532978408754807,\n",
       " 0.4973023448848309,\n",
       " 0.8264995523724261,\n",
       " 0.5470836662481452)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, source_test, torch_device, device_package, decision_boundary=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
